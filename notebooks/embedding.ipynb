{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd2c3410-07a5-48ca-ba84-e21b20d148c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in e:\\anaconda3\\lib\\site-packages (4.67.1)\n",
      "Collecting langchain\n",
      "  Using cached langchain-0.3.26-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting sentence-transformers\n",
      "  Using cached sentence_transformers-5.0.0-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: colorama in e:\\anaconda3\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Collecting langchain-core<1.0.0,>=0.3.66 (from langchain)\n",
      "  Using cached langchain_core-0.3.68-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting langchain-text-splitters<1.0.0,>=0.3.8 (from langchain)\n",
      "  Using cached langchain_text_splitters-0.3.8-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting langsmith>=0.1.17 (from langchain)\n",
      "  Using cached langsmith-0.4.4-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in e:\\anaconda3\\lib\\site-packages (from langchain) (2.11.5)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in e:\\anaconda3\\lib\\site-packages (from langchain) (2.0.34)\n",
      "Requirement already satisfied: requests<3,>=2 in e:\\anaconda3\\lib\\site-packages (from langchain) (2.32.4)\n",
      "Requirement already satisfied: PyYAML>=5.3 in e:\\anaconda3\\lib\\site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in e:\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.52.4)\n",
      "Requirement already satisfied: torch>=1.11.0 in e:\\anaconda3\\lib\\site-packages (from sentence-transformers) (2.5.1)\n",
      "Requirement already satisfied: scikit-learn in e:\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.7.0)\n",
      "Requirement already satisfied: scipy in e:\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.15.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in e:\\anaconda3\\lib\\site-packages (from sentence-transformers) (0.32.5)\n",
      "Requirement already satisfied: Pillow in e:\\anaconda3\\lib\\site-packages (from sentence-transformers) (10.4.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in e:\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: filelock in e:\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in e:\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in e:\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in e:\\anaconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (8.2.3)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in e:\\anaconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (1.33)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in e:\\anaconda3\\lib\\site-packages (from langsmith>=0.1.17->langchain) (0.27.0)\n",
      "Collecting orjson<4.0.0,>=3.9.14 (from langsmith>=0.1.17->langchain)\n",
      "  Using cached orjson-3.10.18-cp312-cp312-win_amd64.whl.metadata (43 kB)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in e:\\anaconda3\\lib\\site-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in e:\\anaconda3\\lib\\site-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in e:\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in e:\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in e:\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in e:\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in e:\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in e:\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in e:\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (2025.4.26)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in e:\\anaconda3\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.1)\n",
      "Requirement already satisfied: setuptools in e:\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (78.1.1)\n",
      "Collecting sympy==1.13.1 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: networkx in e:\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
      "Requirement already satisfied: jinja2 in e:\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in e:\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.17 in e:\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in e:\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.9.11)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in e:\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in e:\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in e:\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in e:\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Requirement already satisfied: anyio in e:\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (4.2.0)\n",
      "Requirement already satisfied: httpcore==1.* in e:\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.2)\n",
      "Requirement already satisfied: sniffio in e:\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in e:\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in e:\\anaconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain) (2.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in e:\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Using cached langchain-0.3.26-py3-none-any.whl (1.0 MB)\n",
      "Using cached sentence_transformers-5.0.0-py3-none-any.whl (470 kB)\n",
      "Using cached langchain_core-0.3.68-py3-none-any.whl (441 kB)\n",
      "Using cached langchain_text_splitters-0.3.8-py3-none-any.whl (32 kB)\n",
      "Using cached langsmith-0.4.4-py3-none-any.whl (367 kB)\n",
      "Downloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "   ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/6.2 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/6.2 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.5/6.2 MB 799.2 kB/s eta 0:00:08\n",
      "   --- ------------------------------------ 0.5/6.2 MB 799.2 kB/s eta 0:00:08\n",
      "   --- ------------------------------------ 0.5/6.2 MB 799.2 kB/s eta 0:00:08\n",
      "   ------ --------------------------------- 1.0/6.2 MB 839.3 kB/s eta 0:00:07\n",
      "   -------- ------------------------------- 1.3/6.2 MB 894.7 kB/s eta 0:00:06\n",
      "   ---------- ----------------------------- 1.6/6.2 MB 921.7 kB/s eta 0:00:06\n",
      "   ---------- ----------------------------- 1.6/6.2 MB 921.7 kB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 1.8/6.2 MB 875.3 kB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 1.8/6.2 MB 875.3 kB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 1.8/6.2 MB 875.3 kB/s eta 0:00:05\n",
      "   --------------- ------------------------ 2.4/6.2 MB 838.9 kB/s eta 0:00:05\n",
      "   --------------- ------------------------ 2.4/6.2 MB 838.9 kB/s eta 0:00:05\n",
      "   --------------- ------------------------ 2.4/6.2 MB 838.9 kB/s eta 0:00:05\n",
      "   --------------- ------------------------ 2.4/6.2 MB 838.9 kB/s eta 0:00:05\n",
      "   ------------------ --------------------- 2.9/6.2 MB 773.3 kB/s eta 0:00:05\n",
      "   ------------------ --------------------- 2.9/6.2 MB 773.3 kB/s eta 0:00:05\n",
      "   ------------------ --------------------- 2.9/6.2 MB 773.3 kB/s eta 0:00:05\n",
      "   -------------------- ------------------- 3.1/6.2 MB 726.5 kB/s eta 0:00:05\n",
      "   -------------------- ------------------- 3.1/6.2 MB 726.5 kB/s eta 0:00:05\n",
      "   ----------------------- ---------------- 3.7/6.2 MB 781.8 kB/s eta 0:00:04\n",
      "   --------------------------- ------------ 4.2/6.2 MB 850.2 kB/s eta 0:00:03\n",
      "   --------------------------- ------------ 4.2/6.2 MB 850.2 kB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 4.5/6.2 MB 831.0 kB/s eta 0:00:03\n",
      "   ------------------------------ --------- 4.7/6.2 MB 831.6 kB/s eta 0:00:02\n",
      "   ------------------------------ --------- 4.7/6.2 MB 831.6 kB/s eta 0:00:02\n",
      "   ------------------------------ --------- 4.7/6.2 MB 831.6 kB/s eta 0:00:02\n",
      "   --------------------------------- ------ 5.2/6.2 MB 843.3 kB/s eta 0:00:02\n",
      "   --------------------------------- ------ 5.2/6.2 MB 843.3 kB/s eta 0:00:02\n",
      "   --------------------------------- ------ 5.2/6.2 MB 843.3 kB/s eta 0:00:02\n",
      "   --------------------------------- ------ 5.2/6.2 MB 843.3 kB/s eta 0:00:02\n",
      "   -------------------------------------- - 6.0/6.2 MB 858.3 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 6.0/6.2 MB 858.3 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.2/6.2 MB 835.5 kB/s eta 0:00:00\n",
      "Using cached orjson-3.10.18-cp312-cp312-win_amd64.whl (134 kB)\n",
      "Installing collected packages: sympy, orjson, langsmith, langchain-core, sentence-transformers, langchain-text-splitters, langchain\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.13.2\n",
      "    Uninstalling sympy-1.13.2:\n",
      "      Successfully uninstalled sympy-1.13.2\n",
      "Successfully installed langchain-0.3.26 langchain-core-0.3.68 langchain-text-splitters-0.3.8 langsmith-0.4.4 orjson-3.10.18 sentence-transformers-5.0.0 sympy-1.13.1\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm langchain sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d99000-54ae-4825-b12e-3af6eb210437",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "import faiss\n",
    "\n",
    "from tqdm.notebook import tqd\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80363ef3-4930-4fd9-b654-c723cb2fa895",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"../data/filtered_complaints.csv\"\n",
    "VECTOR_STORE_DIR = \"../vector_store\"\n",
    "CHUNK_SIZE = 1000\n",
    "CHUNK_OVERLAP = 250\n",
    "EMBEDDING_MODEL_NAME = \"all-MiniLM-L6-v2\"\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d992df58-3c0f-4477-a314-1b580c3e6db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Loading dataset...\n",
      "[+] Initializing text splitter and model...\n",
      "[+] Using device: cpu\n",
      "[+] Chunking text and collecting metadata...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 104/104 [00:00<00:00, 554.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Total chunks: 691\n",
      "[+] Generating embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52cd78c44c5e41f1bceb500f9ef3a8b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Building FAISS index...\n",
      "[✓] Saved FAISS index and metadata for 691 chunks.\n"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# Load data\n",
    "# -------------------------\n",
    "print(\"[+] Loading dataset...\")\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "assert \"cleaned_narrative\" in df.columns, \"Missing 'cleaned_narrative' column\"\n",
    "\n",
    "# -------------------------\n",
    "# Initialize components\n",
    "# -------------------------\n",
    "print(\"[+] Initializing text splitter and model...\")\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=CHUNK_OVERLAP\n",
    ")\n",
    "\n",
    "device = 'cpu'\n",
    "print(f\"[+] Using device: {device}\")\n",
    "\n",
    "model = SentenceTransformer(EMBEDDING_MODEL_NAME, device=device)\n",
    "\n",
    "# -------------------------\n",
    "# Chunk narratives and collect metadata\n",
    "# -------------------------\n",
    "print(\"[+] Chunking text and collecting metadata...\")\n",
    "all_chunks = []\n",
    "metadata_list = []\n",
    "\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    complaint_id = row.get(\"Complaint ID\", idx)\n",
    "    product = row[\"Product\"]\n",
    "    text = row[\"cleaned_narrative\"]\n",
    "\n",
    "    chunks = text_splitter.split_text(text)\n",
    "    for chunk in chunks:\n",
    "        all_chunks.append(chunk)\n",
    "        metadata_list.append({\n",
    "            \"complaint_id\": complaint_id,\n",
    "            \"product\": product,\n",
    "            \"text\": chunk\n",
    "        })\n",
    "\n",
    "print(f\"[+] Total chunks: {len(all_chunks)}\")\n",
    "\n",
    "# -------------------------\n",
    "# Generate embeddings (batch, GPU)\n",
    "# -------------------------\n",
    "print(\"[+] Generating embeddings...\")\n",
    "embeddings = model.encode(\n",
    "    all_chunks,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    show_progress_bar=True,\n",
    "    convert_to_numpy=True,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# Build FAISS index\n",
    "# -------------------------\n",
    "print(\"[+] Building FAISS index...\")\n",
    "embedding_dim = embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(embedding_dim)\n",
    "index.add(embeddings)\n",
    "\n",
    "# -------------------------\n",
    "# Save index and metadata\n",
    "# -------------------------\n",
    "os.makedirs(VECTOR_STORE_DIR, exist_ok=True)\n",
    "\n",
    "faiss.write_index(index, os.path.join(VECTOR_STORE_DIR, \"faiss_index.bin\"))\n",
    "with open(os.path.join(VECTOR_STORE_DIR, \"metadata.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(metadata_list, f)\n",
    "\n",
    "print(f\"[✓] Saved FAISS index and metadata for {len(embeddings)} chunks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615c844b-b971-4e7c-a30c-60dad3339d34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
